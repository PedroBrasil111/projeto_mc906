{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c21dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417c0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"features/Kaisa_features.json\")\n",
    "\n",
    "df_expanded = pd.concat([\n",
    "    df[col].apply(pd.Series).add_prefix(f'{col}_')\n",
    "    for col in df.columns\n",
    "], axis=1)\n",
    "\n",
    "for i in range(10):\n",
    "    items_col = f\"{i}_items\"\n",
    "    items_expanded = df_expanded[items_col].apply(pd.Series)\n",
    "    items_expanded.columns = [f\"{items_col}_{j}\" for j in range(items_expanded.shape[1])]\n",
    "    df_expanded = pd.concat([df_expanded, items_expanded], axis=1)\n",
    "    df_expanded.drop(f\"{i}_items\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6662cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches: 2756\n",
      "Number of matches with correct lane setup: 2679\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total matches: {len(df_expanded)}\")\n",
    "df_same_lane = df_expanded[\n",
    "    (df_expanded[\"0_lane\"] == \"TOP\")     & (df_expanded[\"1_lane\"] == \"TOP\") &\n",
    "    (df_expanded[\"2_lane\"] == \"JUNGLE\")  & (df_expanded[\"3_lane\"] == \"JUNGLE\") &\n",
    "    (df_expanded[\"4_lane\"] == \"MIDDLE\")  & (df_expanded[\"5_lane\"] == \"MIDDLE\") &\n",
    "    (df_expanded[\"6_lane\"] == \"BOTTOM\")  & (df_expanded[\"7_lane\"] == \"BOTTOM\") & \n",
    "    (df_expanded[\"8_lane\"] == \"UTILITY\") & (df_expanded[\"9_lane\"] == \"UTILITY\")\n",
    "]\n",
    "lanes = [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]\n",
    "print(f\"Number of matches with correct lane setup: {len(df_same_lane)}\")\n",
    "df_same_lane.to_csv(\"features/Kaisa_features_expanded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9b3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load champion IDs\n",
    "with open(f\"champions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "    champions = {json_data[champion][\"id\"]: champion for champion in json_data.keys()}\n",
    "\n",
    "# Load Item IDs (Only legendary items)\n",
    "with open(f\"items/items.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "    item_ids = [json_data[str_id][\"id\"] for str_id in json_data.keys() if json_data[str_id][\"tier\"] in [3, 4]]\n",
    "    item_ids = set(item_ids).intersection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns with champion IDs\n",
    "champ_cols = [col for col in df_same_lane.columns if '_championId' in col]\n",
    "\n",
    "# Function to get team for each player column\n",
    "def get_team(col):\n",
    "    player_index = int(col.split('_')[0])\n",
    "    return 100 if player_index < 5 else 200\n",
    "\n",
    "# Prepare empty DataFrames for each team\n",
    "ohe_team100 = pd.DataFrame(0, index=df_same_lane.index, columns=[f\"team100_{cname}\" for cid, cname in champions.items()])\n",
    "ohe_team200 = pd.DataFrame(0, index=df_same_lane.index, columns=[f\"team200_{cname}\" for cid, cname in champions.items()])\n",
    "\n",
    "# Fill OHE\n",
    "for col in champ_cols:\n",
    "    team = get_team(col)\n",
    "    for idx, champ_id in df_same_lane[col].items():\n",
    "        champName = champions[champ_id]\n",
    "        if team == 100:\n",
    "            ohe_team100.loc[idx, f\"team100_{champName}\"] = 1\n",
    "        else:\n",
    "            ohe_team200.loc[idx, f\"team200_{champName}\"] = 1\n",
    "\n",
    "# Concatenate the OHE columns to your DataFrame\n",
    "df_ohe = pd.concat([df_same_lane, ohe_team100, ohe_team200], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7840ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "champId = 145  # Kai'Sa ID\n",
    "\n",
    "# Find in which column (player index) Kai'Sa appears in each row\n",
    "kai_col = df_ohe[[f\"{i}_championId\" for i in range(10)]].eq(champId)\n",
    "\n",
    "# Get the player index for Kai'Sa for each row\n",
    "kai_player_idx = kai_col.idxmax(axis=1)  # Will give like '3_championId', '7_championId', etc.\n",
    "\n",
    "# Filter only rows where Kai'Sa is present\n",
    "has_kai = kai_col.any(axis=1)\n",
    "df_kai = df_ohe.loc[has_kai].copy()\n",
    "\n",
    "# Extract stats for Kai'Sa player in each row\n",
    "def get_stat(row, stat):\n",
    "    player_prefix = row['kai_player'].split('_')[0]\n",
    "    return row[f\"{player_prefix}_{stat}\"]\n",
    "\n",
    "df_kai['kai_player'] = kai_player_idx[has_kai]\n",
    "\n",
    "# Example stats you want to collect\n",
    "for stat in ['kills', 'deaths', 'assists', 'goldEarned', 'level',] :\n",
    "    df_kai[f'kaisa_{stat}'] = df_kai.apply(lambda row: get_stat(row, stat), axis=1)\n",
    "\n",
    "for item in [f\"items_{i}\" for i in range(6)]:\n",
    "    df_kai[f'kaisa_{item}'] = df_kai.apply(lambda row: get_stat(row, item), axis=1)\n",
    "    df_kai[f'kaisa_{item}'] = df_kai.apply(lambda row: row[f\"kaisa_{item}\"] if row[f\"kaisa_{item}\"] in item_ids else 0, axis=1)\n",
    "\n",
    "# Example: Calculate KDA for Kai'Sa\n",
    "df_kai['kaisa_kda'] = (df_kai['kaisa_kills'] + df_kai['kaisa_assists']) / df_kai['kaisa_deaths'].replace(0, np.nan)\n",
    "df_kai['good_gold'] = df_kai['kaisa_goldEarned'] >= df_ohe.loc[:, [f\"{i}_goldEarned\" for i in range(10)]].mean(axis=1)\n",
    "\n",
    "# Drop helper columns if you want\n",
    "df_kai = df_kai[(df_kai['kaisa_kda'] >= 0.8) & (df_kai['good_gold'])]\n",
    "\n",
    "df_kai = df_kai.drop([f\"{i}_kills\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_deaths\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_assists\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_teamId\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_championId\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_lane\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_level\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_matchResult\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_goldEarned\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_0\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_1\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_2\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_3\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_4\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_5\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop([f\"{i}_items_6\" for i in range(10)], axis=1)\n",
    "df_kai = df_kai.drop(['kaisa_kills', 'kaisa_kda', 'good_gold', 'kaisa_deaths', 'kaisa_assists', 'kaisa_goldEarned', 'kaisa_level'], axis=1)\n",
    "df_kai = df_kai.drop(columns=['kai_player'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf13fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_kai.drop([col for col in df_kai if \"items\" not in col], axis=1)\n",
    "y = df_kai[[col for col in df_kai if \"items\" in col]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921fa329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0, 3087, 6676,    0,    0],\n",
       "       [   0, 3046, 3087,    0, 6676,    0],\n",
       "       [   0, 3036, 3087, 3046,    0,    0],\n",
       "       ...,\n",
       "       [6655,    0, 3042,    0,    0, 3115],\n",
       "       [6676,    0,    0, 3032,    0, 3033],\n",
       "       [   0, 6676,    0, 3032,    0, 6675]], shape=(1897, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_slots = 7\n",
    "num_items = len(item_ids) + 1  # If you want to include 0 (empty slot) as a class\n",
    "\n",
    "# Convert to numpy\n",
    "y_class = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a8ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot 0 accuracy: 0.859\n",
      "Slot 1 accuracy: 0.855\n",
      "Slot 2 accuracy: 0.901\n",
      "Slot 3 accuracy: 0.877\n",
      "Slot 4 accuracy: 0.854\n",
      "Slot 5 accuracy: 0.883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.5, random_state=42)\n",
    "\n",
    "# Random Forest (with one classifier per slot)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_rf = MultiOutputClassifier(rf)\n",
    "\n",
    "# Train\n",
    "multi_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = multi_rf.predict(X_test)\n",
    "\n",
    "# Accuracy per slot\n",
    "for slot in range(6):\n",
    "    acc = accuracy_score(y_test[:, slot], y_pred[:, slot])\n",
    "    print(f\"Slot {slot} accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d918a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b8924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a6877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
